{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set gpu number to 2\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TemporalDataset(Dataset):\n",
    "    def __init__(self, split, dataset='fashion_mnist', transform=None, img_to_timesteps_transforms=None):\n",
    "        \"\"\"\n",
    "        Initializes the FashionMNISTNoisyDataset\n",
    "        :param split: 'train' or 'test'\n",
    "        :param dataset: name of the dataset\n",
    "            must be one of the datasets in the huggingface datasets package\n",
    "        :param transform: transforms to apply to the images\n",
    "        :param img_to_timesteps_transforms: list of functions\n",
    "            for every desired timestep, there should be a function in the list that converts\n",
    "            the image to the desired format\n",
    "        \"\"\"\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.dataset = load_dataset(dataset, split=split)\n",
    "        input_col_name = 'img' if 'img' in self.dataset.column_names else 'image'  # because different datasets have different names\n",
    "        self.data, self.targets = self.dataset[input_col_name], self.dataset['label']\n",
    "        self.img_to_timesteps_transforms = img_to_timesteps_transforms\n",
    "        \n",
    "    def int_to_coordinate(self, index):\n",
    "        if index == 0:\n",
    "            return 0, 0\n",
    "        elif index == 1:\n",
    "            return 0, 28\n",
    "        elif index == 2:\n",
    "            return 28, 0\n",
    "        elif index == 3:\n",
    "            return 28, 28\n",
    "        else:\n",
    "            raise ValueError('index must be between 0 and 3')\n",
    "        \n",
    "    def adjust_contrast(self, img, contrast):\n",
    "        mean = img.mean()\n",
    "        img = (img - mean) * contrast #+ mean\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_timesteps = list()\n",
    "        labels = list()\n",
    "        \n",
    "        # sample 4 ints between 0 and 20\n",
    "        img_onsets = np.random.randint(0, 20, 3)\n",
    "        img_onsets = np.append(img_onsets, 0)\n",
    "        img_locations = np.random.choice(4, 4, replace=False)\n",
    "        prev_img = torch.zeros((1, 28 * 2, 28 * 2)) + 0.5\n",
    "        n_image = 0\n",
    "        for i, trans_func in enumerate(self.img_to_timesteps_transforms):\n",
    "            \n",
    "            if i in img_onsets:\n",
    "                # count number of times it's in the list\n",
    "                count = img_onsets.tolist().count(i)\n",
    "                cur_labels = list()\n",
    "                cur_contrasts = list()\n",
    "                for j in range(count):\n",
    "                    # sample random image\n",
    "                    idx = np.random.randint(0, len(self.data))\n",
    "                    img, target = self.data[idx], int(self.targets[idx])\n",
    "                    \n",
    "                    if self.transform is not None:\n",
    "                        img = self.transform(img)\n",
    "                    mask = img > 1e-2\n",
    "                    img = trans_func(img, index, target)\n",
    "                    \n",
    "                    rand_contrast = np.random.uniform(0.1, 1)\n",
    "                    img = self.adjust_contrast(img, rand_contrast)\n",
    "                                        \n",
    "                    new_img = torch.zeros((1, 28 * 2, 28 * 2))\n",
    "                    x, y = self.int_to_coordinate(img_locations[n_image])\n",
    "                    new_img[:, x:x+28, y:y+28] = img * mask\n",
    "                    n_image += 1\n",
    "                    \n",
    "                    cur_labels.append(target)\n",
    "                    cur_contrasts.append(rand_contrast)\n",
    "                    prev_img = prev_img + new_img\n",
    "                labels.append(\n",
    "                    cur_labels[np.array(cur_contrasts).argmax()]\n",
    "                )\n",
    "                \n",
    "                img_timesteps.append(prev_img)\n",
    "            else:\n",
    "                img_timesteps.append(prev_img)\n",
    "                labels.append(labels[-1])                    \n",
    "            \n",
    "        # Stack the augmented images along the timestep dimension\n",
    "        img_timesteps = torch.stack(img_timesteps, dim=0)\n",
    "        labels = torch.tensor(labels)\n",
    "        return img_timesteps, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13374a47d4fc5c3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.5,), (0.5,))\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef95749539394644"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.transforms import Identity\n",
    "\n",
    "eye = Identity()\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    os.sched_setaffinity(0, range(os.cpu_count())) \n",
    "    \n",
    "timestep_transforms = [eye] * 20\n",
    "# Create instances of the Fashion MNIST dataset\n",
    "train_dataset = TemporalDataset('train', transform=transform,\n",
    "                                     img_to_timesteps_transforms=timestep_transforms)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10132be608d283a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=100, worker_init_fn=worker_init_fn)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a608a2384535baba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EvalDataWrapper(Dataset):\n",
    "    \"\"\"Simple Wrapper that adds contrast and repeated noise information to the dataset to power\n",
    "    the evaluation metrics\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, contrast, rep_noise):\n",
    "        self.dataset = dataset\n",
    "        self.contrast = float(contrast)\n",
    "        self.rep_noise = bool(rep_noise)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx]\n",
    "        if type(idx) == int:\n",
    "            idx = torch.tensor([idx])\n",
    "        contrast = torch.full_like(idx, self.contrast, dtype=torch.float)\n",
    "        rep_noise = torch.full_like(idx, self.rep_noise, dtype=torch.bool)\n",
    "        return x, y, contrast, rep_noise\n",
    "test_loader = DataLoader(EvalDataWrapper(train_dataset, contrast=1, rep_noise=False), batch_size=64, shuffle=True, num_workers=100, worker_init_fn=worker_init_fn)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e20a2b298800d75"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Divisive Normalization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f71965603884c145"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from modules.lateral_recurrence import LateralRecurrence\n",
    "from modules.exponential_decay import ExponentialDecay\n",
    "from modules.divisive_norm import DivisiveNorm\n",
    "from modules.divisive_norm_group import DivisiveNormGroup\n",
    "from modules.div_norm_channel import DivisiveNormChannel\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "import json\n",
    "from models.adaptation import Adaptation\n",
    "from models.HookedRecursiveCNN import HookedRecursiveCNN\n",
    "\n",
    "config_path = 'config.json'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "dataset = config[\"dataset\"]\n",
    "if config[\"dataset\"] == 'fashion_mnist':\n",
    "    layer_kwargs = config[\"layer_kwargs_fmnist\"]\n",
    "elif config[\"dataset\"] == 'cifar10':\n",
    "    layer_kwargs = config[\"layer_kwargs_cifar10\"]\n",
    "\n",
    "# Define transforms for data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "logger = CSVLogger(config[\"log_dir\"], name=config[\"log_name\"])\n",
    "\n",
    "if config[\"adaptation_module\"] == 'LateralRecurrence':\n",
    "    adaptation_module = LateralRecurrence\n",
    "    adaptation_kwargs = config[\"adaptation_kwargs_lateral\"]\n",
    "elif config[\"adaptation_module\"] == 'ExponentialDecay':\n",
    "    adaptation_module = ExponentialDecay\n",
    "    adaptation_kwargs = config[\"adaptation_kwargs_additive\"]\n",
    "elif config[\"adaptation_module\"] == 'DivisiveNorm':\n",
    "    adaptation_module = DivisiveNorm\n",
    "    adaptation_kwargs = config[\"adaptation_kwargs_div_norm\"]\n",
    "elif config[\"adaptation_module\"] == 'DivisiveNormGroup':\n",
    "    adaptation_module = DivisiveNormGroup\n",
    "    adaptation_kwargs = config[\"adaptation_kwargs_div_norm_group\"]\n",
    "elif config[\"adaptation_module\"] == 'DivisiveNormChannel':\n",
    "    adaptation_module = DivisiveNormChannel\n",
    "    adaptation_kwargs = config[\"adaptation_kwargs_div_norm_channel\"]\n",
    "else:\n",
    "    raise ValueError(f'Adaptation module {config[\"adaptation_module\"]} not implemented')\n",
    "\n",
    "t_steps = 20\n",
    "\n",
    "num_epoch = 10\n",
    "\n",
    "layer_kwargs = [{'in_channels': 1, 'out_channels': 32, 'kernel_size': 5},\n",
    " {'in_channels': 32, 'out_channels': 32, 'kernel_size': 5},\n",
    " {'in_channels': 32, 'out_channels': 32, 'kernel_size': 3},\n",
    " {'in_channels': 32, 'out_channels': 32, 'kernel_size': 3},\n",
    " {'in_features': 128, 'out_features': 1024}]\n",
    "adaptation_kwargs = [\n",
    "    {\"epsilon\":  1e-8, \"K_init\":  0.2, \"train_K\":  True, \"alpha_init\":  -2.0, \"train_alpha\": True, \"sigma_init\": 0.1, \"train_sigma\": True},\n",
    "    {\"epsilon\":  1e-8, \"K_init\":  1.0, \"train_K\":  False, \"alpha_init\":  -2000000.0, \"train_alpha\": False, \"sigma_init\": 1.0, \"train_sigma\": False},\n",
    "    {\"epsilon\":  1e-8, \"K_init\":  1.0, \"train_K\":  False, \"alpha_init\":  -2000000.0, \"train_alpha\": False, \"sigma_init\": 1.0, \"train_sigma\": False},\n",
    "    {\"epsilon\":  1e-8, \"K_init\":  1.0, \"train_K\":  False, \"alpha_init\":  -2000000.0, \"train_alpha\": False, \"sigma_init\": 1.0, \"train_sigma\": False},\n",
    "    {\"epsilon\":  1e-8, \"K_init\":  1.0, \"train_K\":  False, \"alpha_init\":  0.0, \"train_alpha\": False, \"sigma_init\": 1.0, \"train_sigma\": False}\n",
    "  ]\n",
    "\n",
    "hooked_model = HookedRecursiveCNN(t_steps=t_steps, layer_kwargs=layer_kwargs,\n",
    "                                  adaptation_module=adaptation_module,\n",
    "                                  adaptation_kwargs=adaptation_kwargs, decode_every_timestep=True)\n",
    "model = Adaptation(hooked_model, lr=config[\"lr\"], contrast_metrics=False)\n",
    "\n",
    "contrast = 'random'\n",
    "tb_logger = TensorBoardLogger(\"lightning_logs\",\n",
    "                              name=f'video_{config[\"adaptation_module\"]}_001',\n",
    "                              version=f'videon_{config[\"adaptation_module\"]}_001')\n",
    "\n",
    "# wandb.init(project='ai-thesis', config=config, entity='ai-thesis', name=f'{config[\"log_name\"]}_{config[\"adaptation_module\"]}_c_{contrast}_rep_{repeat_noise}_ep_{num_epoch}')\n",
    "wandb_logger = pl.loggers.WandbLogger(project='ai-thesis', config=config,\n",
    "                                      name=f'video_fmnist_002_{config[\"adaptation_module\"]}_c_{contrast}_ep_{num_epoch}_{config[\"log_name\"]}')\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=num_epoch, logger=wandb_logger)\n",
    "# test_results = trainer.test(model, dataloaders=test_loader)\n",
    "wandb_logger.watch(hooked_model, log='all', log_freq=1000)\n",
    "\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "\n",
    "# test\n",
    "# test_results = trainer.test(model, dataloaders=train_loader)\n",
    "# logger.log_metrics({'contrast': contrast, 'epoch': num_epoch, 'repeat_noise': 'n/a',\n",
    "#                     'test_acc': test_results[0][\"test_acc\"]})\n",
    "# logger.save()\n",
    "\n",
    "trainer.save_checkpoint(\n",
    "    f'learned_models/video_fmnist_{config[\"adaptation_module\"]}_contrast_{contrast}_epoch_{num_epoch}.ckpt')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5de7e15b3a1fee51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Exponential Decay"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8635db20e6538ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create instances of the Fashion MNIST dataset\n",
    "train_dataset = TemporalDataset('train', transform=transform,\n",
    "                                img_to_timesteps_transforms=timestep_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=3)\n",
    "\n",
    "test_loader = DataLoader(EvalDataWrapper(train_dataset, contrast=1, rep_noise=False), batch_size=64, shuffle=True,\n",
    "                         num_workers=3)\n",
    "config_path = 'config.json'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "dataset = config[\"dataset\"]\n",
    "if config[\"dataset\"] == 'fashion_mnist':\n",
    "    layer_kwargs = config[\"layer_kwargs_fmnist\"]\n",
    "elif config[\"dataset\"] == 'cifar10':\n",
    "    layer_kwargs = config[\"layer_kwargs_cifar10\"]\n",
    "    \n",
    "config['adaptation_module'] = 'ExponentialDecay'\n",
    "\n",
    "# Define transforms for data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "logger = CSVLogger(config[\"log_dir\"], name=config[\"log_name\"])\n",
    "\n",
    "if config[\"adaptation_module\"] == 'LateralRecurrence':\n",
    "    adaptation_module = LateralRecurrence\n",
    "    adaptation_kwargs = config[\"adaptation_kwargs_lateral\"]\n",
    "elif config[\"adaptation_module\"] == 'ExponentialDecay':\n",
    "    adaptation_module = ExponentialDecay\n",
    "    adaptation_kwargs = config[\"adaptation_kwargs_additive\"]\n",
    "elif config[\"adaptation_module\"] == 'DivisiveNorm':\n",
    "    adaptation_module = DivisiveNorm\n",
    "    adaptation_kwargs = config[\"adaptation_kwargs_div_norm\"]\n",
    "elif config[\"adaptation_module\"] == 'DivisiveNormGroup':\n",
    "    adaptation_module = DivisiveNormGroup\n",
    "    adaptation_kwargs = config[\"adaptation_kwargs_div_norm_group\"]\n",
    "elif config[\"adaptation_module\"] == 'DivisiveNormChannel':\n",
    "    adaptation_module = DivisiveNormChannel\n",
    "    adaptation_kwargs = config[\"adaptation_kwargs_div_norm_channel\"]\n",
    "else:\n",
    "    raise ValueError(f'Adaptation module {config[\"adaptation_module\"]} not implemented')\n",
    "\n",
    "t_steps = 20\n",
    "\n",
    "num_epoch = 10\n",
    "\n",
    "layer_kwargs = [{'in_channels': 1, 'out_channels': 32, 'kernel_size': 5},\n",
    "                {'in_channels': 32, 'out_channels': 32, 'kernel_size': 5},\n",
    "                {'in_channels': 32, 'out_channels': 32, 'kernel_size': 3},\n",
    "                {'in_channels': 32, 'out_channels': 32, 'kernel_size': 3},\n",
    "                {'in_features': 128, 'out_features': 1024}]\n",
    "adaptation_kwargs = [\n",
    "    {\"alpha_init\":  0.5, \"train_alpha\": True, \"beta_init\": 1, \"train_beta\": True},\n",
    "    {\"alpha_init\":  1.0, \"train_alpha\": False, \"beta_init\": 1, \"train_beta\": False},\n",
    "    {\"alpha_init\":  1.0, \"train_alpha\": False, \"beta_init\": 1, \"train_beta\": False},\n",
    "    {\"alpha_init\":  1.0, \"train_alpha\": False, \"beta_init\": 1, \"train_beta\": False},\n",
    "    {\"alpha_init\":  1.0, \"train_alpha\": False, \"beta_init\": 1, \"train_beta\": False}\n",
    "  ]\n",
    "\n",
    "hooked_model = HookedRecursiveCNN(t_steps=t_steps, layer_kwargs=layer_kwargs,\n",
    "                                  adaptation_module=adaptation_module,\n",
    "                                  adaptation_kwargs=adaptation_kwargs, decode_every_timestep=True)\n",
    "model = Adaptation(hooked_model, lr=config[\"lr\"], contrast_metrics=False)\n",
    "\n",
    "contrast = 'random'\n",
    "tb_logger = TensorBoardLogger(\"lightning_logs\",\n",
    "                              name=f'video_{config[\"adaptation_module\"]}_001',\n",
    "                              version=f'videon_{config[\"adaptation_module\"]}_001')\n",
    "\n",
    "# wandb.init(project='ai-thesis', config=config, entity='ai-thesis', name=f'{config[\"log_name\"]}_{config[\"adaptation_module\"]}_c_{contrast}_rep_{repeat_noise}_ep_{num_epoch}')\n",
    "wandb_logger = pl.loggers.WandbLogger(project='ai-thesis', config=config,\n",
    "                                      name=f'video_fmnist_002_{config[\"adaptation_module\"]}_c_{contrast}_ep_{num_epoch}_{config[\"log_name\"]}')\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=num_epoch, logger=wandb_logger)\n",
    "# test_results = trainer.test(model, dataloaders=test_loader)\n",
    "wandb_logger.watch(hooked_model, log='all', log_freq=1000)\n",
    "\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "\n",
    "# test\n",
    "# test_results = trainer.test(model, dataloaders=train_loader)\n",
    "# logger.log_metrics({'contrast': contrast, 'epoch': num_epoch, 'repeat_noise': 'n/a',\n",
    "#                     'test_acc': test_results[0][\"test_acc\"]})\n",
    "# logger.save()\n",
    "\n",
    "trainer.save_checkpoint(\n",
    "    f'learned_models/video_fmnist_{config[\"adaptation_module\"]}_contrast_{contrast}_epoch_{num_epoch}.ckpt')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60859f5ad389104e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train no-time baseline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "223917e54046dde0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from modules.lateral_recurrence import LateralRecurrence\n",
    "from modules.exponential_decay import ExponentialDecay\n",
    "from modules.divisive_norm import DivisiveNorm\n",
    "from modules.divisive_norm_group import DivisiveNormGroup\n",
    "from modules.div_norm_channel import DivisiveNormChannel\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "import json\n",
    "from models.adaptation import Adaptation\n",
    "from models.HookedRecursiveCNN import HookedRecursiveCNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61771d1df45b7e3e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create instances of the Fashion MNIST dataset\n",
    "train_dataset = TemporalDataset('train', transform=transform,\n",
    "                                img_to_timesteps_transforms=timestep_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=3)\n",
    "\n",
    "test_loader = DataLoader(EvalDataWrapper(train_dataset, contrast=1, rep_noise=False), batch_size=64, shuffle=True,\n",
    "                         num_workers=3)\n",
    "config_path = 'config.json'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "dataset = config[\"dataset\"]\n",
    "if config[\"dataset\"] == 'fashion_mnist':\n",
    "    layer_kwargs = config[\"layer_kwargs_fmnist\"]\n",
    "elif config[\"dataset\"] == 'cifar10':\n",
    "    layer_kwargs = config[\"layer_kwargs_cifar10\"]\n",
    "    \n",
    "config['adaptation_module'] = 'ExponentialDecay'\n",
    "\n",
    "# Define transforms for data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "logger = CSVLogger(config[\"log_dir\"], name=config[\"log_name\"])\n",
    "\n",
    "if config[\"adaptation_module\"] == 'LateralRecurrence':\n",
    "    adaptation_module = LateralRecurrence\n",
    "    adaptation_kwargs = config[\"adaptation_kwargs_lateral\"]\n",
    "elif config[\"adaptation_module\"] == 'ExponentialDecay':\n",
    "    adaptation_module = ExponentialDecay\n",
    "    adaptation_kwargs = config[\"adaptation_kwargs_additive\"]\n",
    "elif config[\"adaptation_module\"] == 'DivisiveNorm':\n",
    "    adaptation_module = DivisiveNorm\n",
    "    adaptation_kwargs = config[\"adaptation_kwargs_div_norm\"]\n",
    "elif config[\"adaptation_module\"] == 'DivisiveNormGroup':\n",
    "    adaptation_module = DivisiveNormGroup\n",
    "    adaptation_kwargs = config[\"adaptation_kwargs_div_norm_group\"]\n",
    "elif config[\"adaptation_module\"] == 'DivisiveNormChannel':\n",
    "    adaptation_module = DivisiveNormChannel\n",
    "    adaptation_kwargs = config[\"adaptation_kwargs_div_norm_channel\"]\n",
    "else:\n",
    "    raise ValueError(f'Adaptation module {config[\"adaptation_module\"]} not implemented')\n",
    "\n",
    "t_steps = 20\n",
    "\n",
    "num_epoch = 10\n",
    "\n",
    "layer_kwargs = [{'in_channels': 1, 'out_channels': 32, 'kernel_size': 5},\n",
    "                {'in_channels': 32, 'out_channels': 32, 'kernel_size': 5},\n",
    "                {'in_channels': 32, 'out_channels': 32, 'kernel_size': 3},\n",
    "                {'in_channels': 32, 'out_channels': 32, 'kernel_size': 3},\n",
    "                {'in_features': 128, 'out_features': 1024}]\n",
    "adaptation_kwargs = [\n",
    "    {\"alpha_init\":  1.0, \"train_alpha\": False, \"beta_init\": 1, \"train_beta\": False},\n",
    "    {\"alpha_init\":  1.0, \"train_alpha\": False, \"beta_init\": 1, \"train_beta\": False},\n",
    "    {\"alpha_init\":  1.0, \"train_alpha\": False, \"beta_init\": 1, \"train_beta\": False},\n",
    "    {\"alpha_init\":  1.0, \"train_alpha\": False, \"beta_init\": 1, \"train_beta\": False},\n",
    "    {\"alpha_init\":  1.0, \"train_alpha\": False, \"beta_init\": 1, \"train_beta\": False}\n",
    "  ]\n",
    "\n",
    "hooked_model = HookedRecursiveCNN(t_steps=t_steps, layer_kwargs=layer_kwargs,\n",
    "                                  adaptation_module=adaptation_module,\n",
    "                                  adaptation_kwargs=adaptation_kwargs, decode_every_timestep=True)\n",
    "model = Adaptation(hooked_model, lr=config[\"lr\"], contrast_metrics=False)\n",
    "\n",
    "contrast = 'random'\n",
    "tb_logger = TensorBoardLogger(\"lightning_logs\",\n",
    "                              name=f'video_{config[\"adaptation_module\"]}_001',\n",
    "                              version=f'baseline_video_{config[\"adaptation_module\"]}_001')\n",
    "\n",
    "# wandb.init(project='ai-thesis', config=config, entity='ai-thesis', name=f'{config[\"log_name\"]}_{config[\"adaptation_module\"]}_c_{contrast}_rep_{repeat_noise}_ep_{num_epoch}')\n",
    "wandb_logger = pl.loggers.WandbLogger(project='ai-thesis', config=config,\n",
    "                                      name=f'baseline_video_fmnist_002_{config[\"adaptation_module\"]}_c_{contrast}_ep_{num_epoch}_{config[\"log_name\"]}')\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=num_epoch, logger=wandb_logger)\n",
    "# test_results = trainer.test(model, dataloaders=test_loader)\n",
    "wandb_logger.watch(hooked_model, log='all', log_freq=1000)\n",
    "\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "\n",
    "# test\n",
    "# test_results = trainer.test(model, dataloaders=train_loader)\n",
    "# logger.log_metrics({'contrast': contrast, 'epoch': num_epoch, 'repeat_noise': 'n/a',\n",
    "#                     'test_acc': test_results[0][\"test_acc\"]})\n",
    "# logger.save()\n",
    "\n",
    "trainer.save_checkpoint(\n",
    "    f'learned_models/baseline_video_fmnist_{config[\"adaptation_module\"]}_contrast_{contrast}_epoch_{num_epoch}.ckpt')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e47ea7243269c58"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f6c3dcf3e4da6be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x.shape, y.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65ea2d5e719d421c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "int_to_label = {\n",
    "    0: 'T-shirt',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle boot'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1542d2742e4f9145"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrow\n",
    "\n",
    "idx = 7\n",
    "sample = x[idx].permute((0, 2, 3, 1))\n",
    "fig, axes = plt.subplots(1, 20, figsize=(20, 4))  # Adjusted figsize to make it wider\n",
    "fig.subplots_adjust(wspace=0.02)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(sample[i], cmap='gray', vmin=0.4, vmax=1)  # Set colormap to greyscale\n",
    "    ax.set_xlabel(int_to_label[int(y[idx, i])], rotation=25, labelpad=10, fontsize=14)  # Rotate and position labels\n",
    "    ax.set_xticks([])  # Remove x ticks\n",
    "    ax.set_yticks([])  # Remove y ticks\n",
    "\n",
    "# Drawing a long arrow\n",
    "arrow = FancyArrow(0.15, 0.2, 0.65, 0, width=0.01, color='black', transform=fig.transFigure, clip_on=False)\n",
    "fig.add_artist(arrow)\n",
    "\n",
    "plt.savefig(\"figures/attn_001.svg\", format='svg')\n",
    "plt.savefig(\"figures/attn_001.png\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "678f41fc0c845b89"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load pretrained models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26b41c78f214432f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from modules.lateral_recurrence import LateralRecurrence\n",
    "from modules.exponential_decay import ExponentialDecay\n",
    "from modules.divisive_norm import DivisiveNorm\n",
    "from modules.divisive_norm_group import DivisiveNormGroup\n",
    "from modules.div_norm_channel import DivisiveNormChannel\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "import json\n",
    "from models.adaptation import Adaptation\n",
    "from models.HookedRecursiveCNN import HookedRecursiveCNN\n",
    "\n",
    "# HookedRecursiveCNN needs layer_kwargs and div_norm_kwargs to know how to setup the model but the concrete init values are unimportant as they will get overwritten with the pretrained values\n",
    "layer_kwargs = [{'in_channels': 1, 'out_channels': 32, 'kernel_size': 5},\n",
    " {'in_channels': 32, 'out_channels': 32, 'kernel_size': 5},\n",
    " {'in_channels': 32, 'out_channels': 32, 'kernel_size': 3},\n",
    " {'in_channels': 32, 'out_channels': 32, 'kernel_size': 3},\n",
    " {'in_features': 128, 'out_features': 1024}]\n",
    "\n",
    "div_norm_kwargs = [\n",
    "    {\"epsilon\":  1e-8, \"K_init\":  0.2, \"train_K\":  True, \"alpha_init\":  -2.0, \"train_alpha\": True, \"sigma_init\": 0.1, \"train_sigma\": True},\n",
    "    {\"epsilon\":  1e-8, \"K_init\":  1.0, \"train_K\":  False, \"alpha_init\":  -2000000.0, \"train_alpha\": False, \"sigma_init\": 1.0, \"train_sigma\": False},\n",
    "    {\"epsilon\":  1e-8, \"K_init\":  1.0, \"train_K\":  False, \"alpha_init\":  -2000000.0, \"train_alpha\": False, \"sigma_init\": 1.0, \"train_sigma\": False},\n",
    "    {\"epsilon\":  1e-8, \"K_init\":  1.0, \"train_K\":  False, \"alpha_init\":  -2000000.0, \"train_alpha\": False, \"sigma_init\": 1.0, \"train_sigma\": False},\n",
    "    {\"epsilon\":  1e-8, \"K_init\":  1.0, \"train_K\":  False, \"alpha_init\":  0.0, \"train_alpha\": False, \"sigma_init\": 1.0, \"train_sigma\": False}\n",
    "  ]\n",
    "exp_decay_kwargs = [\n",
    "    {\"alpha_init\":  1.0, \"train_alpha\": False, \"beta_init\": 1, \"train_beta\": False},\n",
    "    {\"alpha_init\":  1.0, \"train_alpha\": False, \"beta_init\": 1, \"train_beta\": False},\n",
    "    {\"alpha_init\":  1.0, \"train_alpha\": False, \"beta_init\": 1, \"train_beta\": False},\n",
    "    {\"alpha_init\":  1.0, \"train_alpha\": False, \"beta_init\": 1, \"train_beta\": False},\n",
    "    {\"alpha_init\":  1.0, \"train_alpha\": False, \"beta_init\": 1, \"train_beta\": False}\n",
    "  ]\n",
    "\n",
    "div_norm_cfg = {\n",
    "    't_steps': 20, 'layer_kwargs': layer_kwargs,\n",
    "    'adaptation_module': DivisiveNorm,\n",
    "    'adaptation_kwargs': div_norm_kwargs, 'decode_every_timestep': True\n",
    "}\n",
    "exp_decay_cfg = {\n",
    "    't_steps': 20, 'layer_kwargs': layer_kwargs,\n",
    "    'adaptation_module': ExponentialDecay,\n",
    "    'adaptation_kwargs': exp_decay_kwargs, 'decode_every_timestep': True\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b5b9b2cee339477"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "div_norm_model = HookedRecursiveCNN.load_from_checkpoint('learned_models/video_fmnist_DivisiveNorm_contrast_random_epoch_10.ckpt', div_norm_cfg)\n",
    "exp_decay_model = HookedRecursiveCNN.load_from_checkpoint('learned_models/video_fmnist_ExponentialDecay_contrast_random_epoch_10.ckpt', exp_decay_cfg)\n",
    "baseline_model = HookedRecursiveCNN.load_from_checkpoint('learned_models/baseline_video_fmnist_ExponentialDecay_contrast_random_epoch_10.ckpt', exp_decay_cfg)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aefb4ba268186145"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Divisive Norm.': div_norm_model,\n",
    "    'Additive': exp_decay_model,\n",
    "    'No Adaptation': baseline_model\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bd2f11e951d3cb4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Accuracy per timestep"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c847a944242cc7ef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torchmetrics.functional import accuracy\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.DataFrame({'Model': [], 'Accuracy': [], 'Timestep': []})\n",
    "for name, model in models.items():\n",
    "    j = 0\n",
    "    for x, y, _, _ in tqdm(test_loader):\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        logits = model(x.cuda())\n",
    "        for i in range(20):\n",
    "            l = logits[:, i, :]\n",
    "            t = y[:, i]\n",
    "            preds = torch.argmax(l, dim=1)\n",
    "            acc = accuracy(preds, t, task='multiclass', num_classes=10)\n",
    "            df.loc[len(df)] = [name, float(acc), i]\n",
    "        j += 1\n",
    "        if j > 100:\n",
    "            break\n",
    "df['Timestep'] += 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a547391c662a926c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "sns.set_style('white')\n",
    "sns.lineplot(data=df, x='Timestep', y='Accuracy', hue='Model', palette='dark')\n",
    "plt.ylim(0, 1)\n",
    "sns.despine(offset=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figures/attn_002.svg')\n",
    "plt.savefig('figures/attn_002.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c99e383f5336001"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Only one image the whole time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e3938d1488c64a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class OneImageTemporalDataset(Dataset):\n",
    "    def __init__(self, split, dataset='fashion_mnist', transform=None, img_to_timesteps_transforms=None, contrast='random'):\n",
    "        \"\"\"\n",
    "        Initializes the FashionMNISTNoisyDataset\n",
    "        :param split: 'train' or 'test'\n",
    "        :param dataset: name of the dataset\n",
    "            must be one of the datasets in the huggingface datasets package\n",
    "        :param transform: transforms to apply to the images\n",
    "        :param img_to_timesteps_transforms: list of functions\n",
    "            for every desired timestep, there should be a function in the list that converts\n",
    "            the image to the desired format\n",
    "        \"\"\"\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.dataset = load_dataset(dataset, split=split)\n",
    "        input_col_name = 'img' if 'img' in self.dataset.column_names else 'image'  # because different datasets have different names\n",
    "        self.data, self.targets = self.dataset[input_col_name], self.dataset['label']\n",
    "        self.img_to_timesteps_transforms = img_to_timesteps_transforms\n",
    "        self.contrast = contrast\n",
    "\n",
    "    def int_to_coordinate(self, index):\n",
    "        if index == 0:\n",
    "            return 0, 0\n",
    "        elif index == 1:\n",
    "            return 0, 28\n",
    "        elif index == 2:\n",
    "            return 28, 0\n",
    "        elif index == 3:\n",
    "            return 28, 28\n",
    "        else:\n",
    "            raise ValueError('index must be between 0 and 3')\n",
    "\n",
    "    def adjust_contrast(self, img, contrast):\n",
    "        mean = img.mean()\n",
    "        img = (img - mean) * contrast  #+ mean\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        img_timesteps = list()\n",
    "        labels = list()\n",
    "\n",
    "        prev_img = torch.zeros((1, 28 * 2, 28 * 2)) + 0.5\n",
    "        for i, trans_func in enumerate(self.img_to_timesteps_transforms):\n",
    "            if i == 0:\n",
    "                if self.transform is not None:\n",
    "                    img = self.transform(img)\n",
    "                mask = img > 1e-2\n",
    "                img = trans_func(img, index, target)\n",
    "\n",
    "                if self.contrast == 'random':\n",
    "                    rand_contrast = np.random.uniform(0.1, 1)\n",
    "                else:\n",
    "                    rand_contrast = self.contrast\n",
    "                img = self.adjust_contrast(img, rand_contrast)\n",
    "\n",
    "                new_img = torch.zeros((1, 28 * 2, 28 * 2))\n",
    "                x, y = 0, 0\n",
    "                new_img[:, x:x + 28, y:y + 28] = img * mask\n",
    "\n",
    "                prev_img = prev_img + new_img\n",
    "                labels.append(\n",
    "                    target\n",
    "                )\n",
    "\n",
    "                img_timesteps.append(prev_img)\n",
    "            else:\n",
    "                img_timesteps.append(prev_img)\n",
    "                labels.append(labels[-1])\n",
    "\n",
    "                # Stack the augmented images along the timestep dimension\n",
    "        img_timesteps = torch.stack(img_timesteps, dim=0)\n",
    "        labels = torch.tensor(labels)\n",
    "        return img_timesteps, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "from utils.transforms import MeanFlat, RandomRepeatedNoise, Identity\n",
    "from functools import partial\n",
    "\n",
    "eye = Identity()\n",
    "\n",
    "timestep_transforms = [eye] * 20\n",
    "# Create instances of the Fashion MNIST dataset\n",
    "one_image_dataset = OneImageTemporalDataset('train', transform=transform,\n",
    "                                img_to_timesteps_transforms=timestep_transforms)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(one_image_dataset, batch_size=64, shuffle=True, num_workers=3)\n",
    "\n",
    "from utils.visualization import visualize_first_batch_with_timesteps\n",
    "\n",
    "visualize_first_batch_with_timesteps(loader, 8)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "150a0147fe507adf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torchmetrics.functional import accuracy\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Model': [], 'Accuracy': [], 'Timestep': []})\n",
    "for name, model in models.items():\n",
    "    model.cpu()\n",
    "    j = 0\n",
    "    for x, y in tqdm(loader):\n",
    "        logits = model(x)\n",
    "        for i in range(20):\n",
    "            l = logits[:, i, :]\n",
    "            t = y[:, 0]\n",
    "            preds = torch.argmax(l, dim=1)\n",
    "            acc = accuracy(preds, t, task='multiclass', num_classes=10)\n",
    "            df.loc[len(df)] = [name, float(acc), i]\n",
    "        j += 1\n",
    "        if j > 100:\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea5909a235f0d64d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['Timestep'] += 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "430f7e9c9c2273f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "sns.set_style('white')\n",
    "sns.lineplot(data=df, x='Timestep', y='Accuracy', hue='Model', palette='dark')\n",
    "plt.ylim(0, 1)\n",
    "sns.despine(offset=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figures/attn_003.svg')\n",
    "plt.savefig('figures/attn_003.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ed12c9577b125b2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## per contrast"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d51ca2b3cd3d7bde"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "acc_dict = {'Contrast': [], 'Timestep': [], 'Accuracy': [], 'Model': []}\n",
    "for contrast in [0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "    one_image_dataset = OneImageTemporalDataset('test', transform=transform,\n",
    "                                    img_to_timesteps_transforms=timestep_transforms, contrast=contrast)\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    loader = DataLoader(one_image_dataset, batch_size=64, shuffle=False, num_workers=10, worker_init_fn=worker_init_fn)\n",
    "    \n",
    "    j = 0\n",
    "    for x, y in tqdm(loader):\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        for name, model in models.items():\n",
    "            model.cuda()\n",
    "            logits = model(x)\n",
    "            for i in range(20):\n",
    "                l = logits[:, i, :]\n",
    "                t = y[:, 0]\n",
    "                preds = torch.argmax(l, dim=1)\n",
    "                acc = accuracy(preds, t, task='multiclass', num_classes=10)\n",
    "                acc_dict['Contrast'].append(contrast)\n",
    "                acc_dict['Timestep'].append(i)\n",
    "                acc_dict['Accuracy'].append(float(acc.cpu()))\n",
    "                acc_dict['Model'].append(name)\n",
    "        j += 1\n",
    "        if j > 100:\n",
    "            break\n",
    "df = pd.DataFrame(acc_dict)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2cbc4516c595ab5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.Timestep += 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee1c2f8ad88a50e1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "sns.relplot(data=df[df.Model.isin(['Divisive Norm.', 'Additive'])], x='Timestep', y='Accuracy', hue='Contrast', row='Model', height=2.5, aspect=1.5, kind='line')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "sns.despine(offset=5)\n",
    "\n",
    "plt.savefig('figures/attn_011.svg')\n",
    "plt.savefig('figures/attn_011.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da8d5d34b78254c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation Scale"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27ab2e050786ed4f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actv_dict = {'Timestep': [], 'Layer': [], 'Mean': [], 'num_active': [], 'mean_not_null': [], 'Norm': [], 'Model': []}\n",
    "one_image_dataset = OneImageTemporalDataset('test', transform=transform,\n",
    "                                img_to_timesteps_transforms=timestep_transforms, contrast='random')\n",
    "\n",
    "loader = DataLoader(one_image_dataset, batch_size=64, shuffle=True, num_workers=3, pin_memory=True, pin_memory_device='cuda')\n",
    "    \n",
    "for x, y in tqdm(loader):\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    for name, model in models.items():\n",
    "        model.cuda()\n",
    "        logits, cache = model.run_with_cache(x)\n",
    "        for layer in range(4):\n",
    "            for timestep in range(20):\n",
    "                actv = cache[f'hks.adapt_{layer}_{timestep}']\n",
    "                actv_dict['Timestep'].append(timestep)\n",
    "                actv_dict['Layer'].append(layer)\n",
    "                actv_dict['Mean'].append(float(actv.mean()))\n",
    "                actv_dict['num_active'].append(float((actv > 1e-4).sum()))\n",
    "                actv_dict['mean_not_null'].append(float(actv[actv > 1e-4].mean()))   \n",
    "                actv_dict['Norm'].append(float(actv.norm()))\n",
    "                actv_dict['Model'].append(name)\n",
    "\n",
    "actv_df = pd.DataFrame(actv_dict)\n",
    "actv_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee5c9ed46eb5ce9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "sns.lineplot(data=actv_df[actv_df.Layer==0], x='Timestep', y='Mean', hue='Model', palette='dark')\n",
    "sns.despine(offset=5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/attn_006.svg')\n",
    "plt.savefig('figures/attn_006.png')\n",
    "#plt.ylim(0, 0.09)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "baaf4c13937552ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "sns.lineplot(data=actv_df[actv_df.Layer==0], x='Timestep', y='Normalized Activations', hue='Model', palette='dark')\n",
    "sns.despine(offset=5)\n",
    "plt.tight_layout()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.savefig('figures/attn_006.svg')\n",
    "plt.savefig('figures/attn_006.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a18287ec863b621c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actv_dict = {'Timestep': [], 'Layer': [], 'Mean': [], 'num_active': [], 'mean_not_null': [], 'Norm': [], 'Model': [], 'Contrast': []}\n",
    "\n",
    "for contrast in [0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "    one_image_dataset = OneImageTemporalDataset('test', transform=transform,\n",
    "                                    img_to_timesteps_transforms=timestep_transforms, contrast=contrast)\n",
    "    \n",
    "    loader = DataLoader(one_image_dataset, batch_size=64, shuffle=True, num_workers=10, pin_memory=True, pin_memory_device='cuda', worker_init_fn=worker_init_fn)\n",
    "        \n",
    "    for x, y in tqdm(loader):\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        for name, model in models.items():\n",
    "            model.cuda()\n",
    "            logits, cache = model.run_with_cache(x)\n",
    "            for layer in range(4):\n",
    "                for timestep in range(20):\n",
    "                    actv = cache[f'hks.adapt_{layer}_{timestep}']\n",
    "                    actv_dict['Timestep'].append(timestep + 1)\n",
    "                    actv_dict['Layer'].append(layer)\n",
    "                    actv_dict['Mean'].append(float(actv.mean()))\n",
    "                    actv_dict['num_active'].append(float((actv > 1e-4).sum()))\n",
    "                    actv_dict['mean_not_null'].append(float(actv[actv > 1e-4].mean()))   \n",
    "                    actv_dict['Norm'].append(float(actv.norm()))\n",
    "                    actv_dict['Model'].append(name)\n",
    "                    actv_dict['Contrast'].append(contrast)\n",
    "\n",
    "actv_df = pd.DataFrame(actv_dict)\n",
    "actv_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9630b44b1c4243f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actv_df['Normalized Activations'] = actv_df.groupby(['Model', 'Layer'], sort=False).apply(\n",
    "    lambda df: df['Mean'] / df.loc[(df.Timestep == 1) & (df.Contrast==1.0), 'Mean'].mean()).reset_index(level=['Model', 'Layer'], drop=True)\n",
    "sns.set_style('white')\n",
    "sns.relplot(data=actv_df[(actv_df.Layer==1) & (actv_df.Model.isin(['Divisive Norm.', 'Additive']))], \n",
    "            x='Timestep', y='Normalized Activations', hue='Contrast', kind='line', row='Model',\n",
    "            height=2.5, aspect=1.5, legend=False)\n",
    "\n",
    "sns.despine(offset=5)\n",
    "\n",
    "plt.savefig('figures/attn_012.svg')\n",
    "plt.savefig('figures/attn_012.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af21110da3b07a55"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Two images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "178031b92371f257"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TwoImageTemporalDataset(Dataset):\n",
    "    def __init__(self, split, onset_img_2, dataset='fashion_mnist', transform=None, img_to_timesteps_transforms=None, contrast_1='random', contrast_2='random'):\n",
    "        \"\"\"\n",
    "        Initializes the FashionMNISTNoisyDataset\n",
    "        :param split: 'train' or 'test'\n",
    "        :param dataset: name of the dataset\n",
    "            must be one of the datasets in the huggingface datasets package\n",
    "        :param transform: transforms to apply to the images\n",
    "        :param img_to_timesteps_transforms: list of functions\n",
    "            for every desired timestep, there should be a function in the list that converts\n",
    "            the image to the desired format\n",
    "        \"\"\"\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.dataset = load_dataset(dataset, split=split)\n",
    "        input_col_name = 'img' if 'img' in self.dataset.column_names else 'image'  # because different datasets have different names\n",
    "        self.data, self.targets = self.dataset[input_col_name], self.dataset['label']\n",
    "        self.img_to_timesteps_transforms = img_to_timesteps_transforms\n",
    "        self.contrast_1 = contrast_1\n",
    "        self.contrast_2 = contrast_2\n",
    "        self.onset_img_2 = onset_img_2\n",
    "\n",
    "    def int_to_coordinate(self, index):\n",
    "        if index == 0:\n",
    "            return 0, 0\n",
    "        elif index == 1:\n",
    "            return 0, 28\n",
    "        elif index == 2:\n",
    "            return 28, 0\n",
    "        elif index == 3:\n",
    "            return 28, 28\n",
    "        else:\n",
    "            raise ValueError('index must be between 0 and 3')\n",
    "\n",
    "    def adjust_contrast(self, img, contrast):\n",
    "        mean = img.mean()\n",
    "        img = (img - mean) * contrast  #+ mean\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        img_timesteps = list()\n",
    "        labels = list()\n",
    "\n",
    "        prev_img = torch.zeros((1, 28 * 2, 28 * 2)) + 0.5\n",
    "        for i, trans_func in enumerate(self.img_to_timesteps_transforms):\n",
    "            if i == 0:\n",
    "                if self.transform is not None:\n",
    "                    img = self.transform(img)\n",
    "                mask = img > 1e-2\n",
    "                img = trans_func(img, index, target)\n",
    "\n",
    "                if self.contrast_1 == 'random':\n",
    "                    rand_contrast = np.random.uniform(0.1, 1)\n",
    "                else:\n",
    "                    rand_contrast = self.contrast_1\n",
    "                img = self.adjust_contrast(img, rand_contrast)\n",
    "\n",
    "                new_img = torch.zeros((1, 28 * 2, 28 * 2))\n",
    "                x, y = 0, 0\n",
    "                new_img[:, x:x + 28, y:y + 28] = img * mask\n",
    "\n",
    "                prev_img = prev_img + new_img\n",
    "                labels.append(\n",
    "                    target\n",
    "                )\n",
    "\n",
    "                img_timesteps.append(prev_img)\n",
    "            elif i == self.onset_img_2:\n",
    "                index = int(np.random.choice(10000, 1))\n",
    "                img, target = self.data[index], int(self.targets[index])\n",
    "                if self.transform is not None:\n",
    "                    img = self.transform(img)\n",
    "                mask = img > 1e-2\n",
    "                img = trans_func(img, index, target)\n",
    "\n",
    "                if self.contrast_2 == 'random':\n",
    "                    rand_contrast = np.random.uniform(0.1, 1)\n",
    "                else:\n",
    "                    rand_contrast = self.contrast_2\n",
    "                img = self.adjust_contrast(img, rand_contrast)\n",
    "\n",
    "                new_img = torch.zeros((1, 28 * 2, 28 * 2))\n",
    "                x, y = 28, 28\n",
    "                new_img[:, x:x + 28, y:y + 28] = img * mask\n",
    "\n",
    "                prev_img = prev_img + new_img\n",
    "                labels.append(\n",
    "                    target\n",
    "                )\n",
    "\n",
    "                img_timesteps.append(prev_img)\n",
    "            else:\n",
    "                img_timesteps.append(prev_img)\n",
    "                labels.append(labels[-1])\n",
    "\n",
    "                # Stack the augmented images along the timestep dimension\n",
    "        img_timesteps = torch.stack(img_timesteps, dim=0)\n",
    "        labels = torch.tensor(labels)\n",
    "        return img_timesteps, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "from models.noisy_dataloader import NoisyTemporalDataset\n",
    "from utils.transforms import MeanFlat, RandomRepeatedNoise, Identity\n",
    "from functools import partial\n",
    "\n",
    "eye = Identity()\n",
    "\n",
    "timestep_transforms = [eye] * 20\n",
    "# Create instances of the Fashion MNIST dataset\n",
    "two_image_dataset = TwoImageTemporalDataset('train', 5, transform=transform,\n",
    "                                img_to_timesteps_transforms=timestep_transforms)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(two_image_dataset, batch_size=64, shuffle=True, num_workers=3)\n",
    "\n",
    "from utils.visualization import visualize_first_batch_with_timesteps\n",
    "\n",
    "visualize_first_batch_with_timesteps(loader, 8)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bda9868b779291d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc_dict = {'Model': [], 'Timestep': [], 'Accuracy': [], 'Onset': []}\n",
    "for onset in [2, 5, 15]:\n",
    "    one_image_dataset = TwoImageTemporalDataset('test', onset_img_2=onset, transform=transform,\n",
    "                                    img_to_timesteps_transforms=timestep_transforms, contrast_1=0.6, contrast_2='random')        \n",
    "    loader = DataLoader(one_image_dataset, batch_size=64, shuffle=True, num_workers=30)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        for x, y in tqdm(loader):\n",
    "            logits = model(x)\n",
    "            for i in range(20):\n",
    "                l = logits[:, i, :]\n",
    "                t = y[:, i]\n",
    "                preds = torch.argmax(l, dim=1)\n",
    "                acc = accuracy(preds, t, task='multiclass', num_classes=10)\n",
    "                acc_dict['Timestep'].append(i)\n",
    "                acc_dict['Accuracy'].append(float(acc))\n",
    "                acc_dict['Onset'].append(onset)\n",
    "                acc_dict['Model'].append(name)\n",
    "df = pd.DataFrame(acc_dict)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9b5ff484a952873"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "one_image_dataset = TwoImageTemporalDataset('test', onset_img_2=4, transform=transform,\n",
    "                                img_to_timesteps_transforms=timestep_transforms, contrast_1=.8, contrast_2=.8)        \n",
    "loader = DataLoader(one_image_dataset, batch_size=10, shuffle=True, num_workers=0)\n",
    "\n",
    "x, y = next(iter(loader))\n",
    "x = x.cuda()\n",
    "y = y.cuda()\n",
    "_, cache = div_norm_model.run_with_cache(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35564d43958c9199"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx = 3\n",
    "map = 2\n",
    "sample = x[idx].permute((0, 2, 3, 1)).cpu()\n",
    "fig, axes = plt.subplots(ncols=8, nrows=2, figsize=(10, 3))\n",
    "fig.subplots_adjust(wspace=0.03, hspace=0.01)\n",
    "\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    ax.imshow(sample[i], cmap='gray')  # Set colormap to greyscale\n",
    "    ax.set_xticks([]) \n",
    "    ax.set_yticks([]) \n",
    "    \n",
    "for i, ax in enumerate(axes[1]):\n",
    "    ax.imshow(cache[f'hks.adapt_0_{i}'][idx, map].cpu(), vmin=0, vmax=1)\n",
    "    ax.set_xticks([]) \n",
    "    ax.set_yticks([]) \n",
    "    \n",
    "axes[0, 0].set_ylabel('Input', fontsize=14)\n",
    "axes[1, 0].set_ylabel('Feature Map', fontsize=14)\n",
    "    \n",
    "# Drawing a long arrow\n",
    "arrow = FancyArrow(0.15, 0.1, 0.65, 0, width=0.01, color='black', transform=fig.transFigure, clip_on=False)\n",
    "fig.add_artist(arrow)\n",
    "\n",
    "plt.savefig(\"figures/attn_005.svg\", format='svg')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ef7514bad8603f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "one_image_dataset = TwoImageTemporalDataset('test', onset_img_2=2, transform=transform,\n",
    "                                            img_to_timesteps_transforms=timestep_transforms, contrast_1=.8,\n",
    "                                            contrast_2=.8)\n",
    "loader = DataLoader(one_image_dataset, batch_size=10, shuffle=True, num_workers=0)\n",
    "\n",
    "x, y = next(iter(loader))\n",
    "x = x.cuda()\n",
    "y = y.cuda()\n",
    "_, cache = div_norm_model.run_with_cache(x)\n",
    "\n",
    "sample = cache[f'hks.adapt_{0}_{2}'].cpu()\n",
    "fig, ax = plt.subplots(figsize=(2, 2))\n",
    "ax.imshow(sample[0, 9], cmap='gray')\n",
    "ax.add_patch(plt.Rectangle((0, 0), 26, 26, fill=False, edgecolor='red', linestyle='--', linewidth=2))\n",
    "ax.add_patch(plt.Rectangle((26, 26), 25, 25, fill=False, edgecolor='green', linestyle='--', linewidth=2))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.savefig('figures/attn_007.svg')\n",
    "plt.savefig('figures/attn_007.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16a14443faf21b99"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"mako\", 3)\n",
    "g = sns.relplot(data=df, x='Timestep', y='Accuracy', hue='Onset', col='Model', style='Onset', kind='line', height=2.5, palette=palette)\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "sns.despine(offset=5)\n",
    "\n",
    "# Adding vertical dashed lines\n",
    "unique_onsets = df['Onset'].unique()\n",
    "\n",
    "# Loop through each subplot and add vertical lines\n",
    "for model, ax in g.axes_dict.items():\n",
    "    for onset, color in zip(unique_onsets, palette):\n",
    "        ax.axvline(x=onset, color=color, linestyle='--', alpha=0.7)\n",
    "plt.savefig('figures/attn_004.svg')\n",
    "plt.savefig('figures/attn_004.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3709c87fb7cca89"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "acc_dict = {'Contrast first image': [], 'Contrast second image': [], 'Timestep': [], 'Accuracy': [], 'Onset': [], 'Logits novel image': [], 'Model': [], 'Logits first image': []}\n",
    "for onset in [2, 5, 15]:\n",
    "    for contrast_1 in [0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "        for contrast_2 in [0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "            one_image_dataset = TwoImageTemporalDataset('train', onset_img_2=onset, transform=transform,\n",
    "                                            img_to_timesteps_transforms=timestep_transforms, contrast_1=contrast_1, contrast_2=contrast_2)        \n",
    "            loader = DataLoader(one_image_dataset, batch_size=64, shuffle=True, num_workers=10, worker_init_fn=worker_init_fn)\n",
    "            \n",
    "            j = 0\n",
    "            for x, y in tqdm(loader):\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                for name, model in models.items():\n",
    "                    model.cuda()\n",
    "                    logits = model(x)\n",
    "                    for i in range(20):\n",
    "                        l = logits[:, i, :]\n",
    "                        t = y[:, i]\n",
    "                        preds = torch.argmax(l, dim=1)\n",
    "                        acc = accuracy(preds, t, task='multiclass', num_classes=10)\n",
    "                        acc_dict['Contrast first image'].append(contrast_1)\n",
    "                        acc_dict['Contrast second image'].append(contrast_2)\n",
    "                        acc_dict['Timestep'].append(i)\n",
    "                        acc_dict['Accuracy'].append(float(acc))\n",
    "                        acc_dict['Onset'].append(onset)\n",
    "                        # how much higher are logits of the novel image compared to the rest\n",
    "                        acc_dict['Logits novel image'].append(float(l[:, y[:, onset]].mean() - l[:, ~y[:, onset]].mean()))\n",
    "                        acc_dict['Logits first image'].append(float(l[:, y[:, 0]].mean() - l[:, ~y[:, 0]].mean()))\n",
    "                        acc_dict['Model'].append(name)             \n",
    "                        \n",
    "                j += 1\n",
    "                if j > 200:\n",
    "                    break\n",
    "df = pd.DataFrame(acc_dict)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c082d9119025d556"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Receptive Fields"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77dd8ae5d6e108c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actv_df = pd.DataFrame({'Model': [], 'Timestep': [], 'Layer': [], 'Mean': [], 'Norm': [], 'num_active': [], 'mean_not_null': [], 'Onset': [], 'Region': [], 'Contrast': []})\n",
    "for onset in [4]:  # [2, 5, 15]:\n",
    "    for contrast in [0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "        two_image_dataset = TwoImageTemporalDataset('test', onset_img_2=onset, transform=transform,\n",
    "                                        img_to_timesteps_transforms=timestep_transforms, contrast_1=0.6, contrast_2=contrast)        \n",
    "        loader = DataLoader(two_image_dataset, batch_size=100, shuffle=False, num_workers=10, worker_init_fn=worker_init_fn)\n",
    "        j = 0\n",
    "        for x, y in tqdm(loader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            for name, model in models.items():\n",
    "                model.cuda()\n",
    "                logits, cache = model.run_with_cache(x)\n",
    "                for layer in range(4):\n",
    "                    for timestep in range(20):\n",
    "                        actv = cache[f'hks.adapt_{layer}_{timestep}'] # batch channel width height\n",
    "                        width = actv.shape[-1]\n",
    "                        upper_left = actv[..., :width // 2, :width // 2]\n",
    "                        lower_right = actv[..., width // 2:, width // 2:]\n",
    "                        for region in ['ul', 'lr', 'both']:\n",
    "                            if region == 'ul':\n",
    "                                cur_actv = upper_left\n",
    "                            elif region == 'lr':\n",
    "                                cur_actv = lower_right\n",
    "                            elif region == 'both':\n",
    "                                cur_actv = actv\n",
    "                            else:\n",
    "                                raise ValueError('region can only be ul, lr, or both')\n",
    "                                \n",
    "                            actv_df.loc[len(actv_df)] = [\n",
    "                                name, timestep, layer, float(cur_actv.mean()), float(cur_actv.norm()),\n",
    "                                float((cur_actv > 1e-4).sum()), float(cur_actv[cur_actv > 1e-4].mean()),\n",
    "                                onset, region, contrast\n",
    "                            ]\n",
    "            j += 1\n",
    "            if j > 40:\n",
    "                break\n",
    "actv_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c22ddce5fac015f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "palette = ['red', 'green']\n",
    "sns.relplot(data=actv_df[(actv_df.Layer==1) & (actv_df.Region.isin(['ul', 'lr']))], x='Timestep', y='Mean', hue='Region', style='Onset', col='Model', kind='line', height=2.5, facet_kws={'sharey': False}, palette=palette)\n",
    "sns.despine(offset=5)\n",
    "\n",
    "plt.savefig('figures/attn_008.svg')\n",
    "plt.savefig('figures/attn_008.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0f69b2950d40bcb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actv_dict = {'contrast': [], 'timestep': [], 'layer': [], 'mean': [], 'num_active': [], 'mean_not_null': [], 'onset': [], 'region': [], 'model': []}\n",
    "for onset in [4]:\n",
    "    for contrast in [0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "        two_image_dataset = TwoImageTemporalDataset('test', onset_img_2=onset, transform=transform,\n",
    "                                        img_to_timesteps_transforms=timestep_transforms, contrast_1=0.6, contrast_2=contrast)        \n",
    "        loader = DataLoader(two_image_dataset, batch_size=50, shuffle=False, num_workers=10, worker_init_fn=worker_init_fn)\n",
    "        \n",
    "        j = 0\n",
    "        for x, y in tqdm(loader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            for name, model in models.items():\n",
    "                model.cuda()\n",
    "                logits, cache = model.run_with_cache(x)\n",
    "                for layer in range(4):\n",
    "                    for timestep in range(20):\n",
    "                        actv = cache[f'hks.adapt_{layer}_{timestep}'] # batch channel width height\n",
    "                        width = actv.shape[-1]\n",
    "                        upper_left = actv[..., :width // 2, :width // 2]\n",
    "                        lower_right = actv[..., width // 2:, width // 2:]\n",
    "                        for region in ['ul', 'lr', 'both']:\n",
    "                            actv_dict['contrast'].append(contrast)\n",
    "                            actv_dict['timestep'].append(timestep)\n",
    "                            actv_dict['layer'].append(layer)\n",
    "                            if region == 'ul':\n",
    "                                cur_actv = upper_left\n",
    "                            elif region == 'lr':\n",
    "                                cur_actv = lower_right\n",
    "                            elif region == 'both':\n",
    "                                cur_actv = actv\n",
    "                            else:\n",
    "                                raise ValueError('region can only be ul, lr, or both')\n",
    "                                \n",
    "                            actv_dict['mean'].append(float(cur_actv.mean().cpu()))\n",
    "                            actv_dict['num_active'].append(float((cur_actv > 1e-4).sum().cpu()))\n",
    "                            actv_dict['mean_not_null'].append(float(cur_actv[cur_actv > 1e-4].mean().cpu()))      \n",
    "                            actv_dict['onset'].append(onset)\n",
    "                            actv_dict['region'].append(region)\n",
    "                            actv_dict['model'].append(name)\n",
    "                    \n",
    "                    \n",
    "            j += 1\n",
    "            if j > 100:\n",
    "                break\n",
    "actv_df = pd.DataFrame(actv_dict)\n",
    "actv_df\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fcf71edb202675a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actv_df['mean'] = [float(val) for val in actv_df['mean']]\n",
    "actv_df['num_active'] = [float(val) for val in actv_df['num_active']]\n",
    "actv_df['mean_not_null'] = [float(val) for val in actv_df['mean_not_null']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37cb3e231c4c7b78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.relplot(data=actv_df[actv_df.region=='both'], x='timestep', y='mean', hue='contrast', kind='line', col='layer', row='onset')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fee675f1de4836e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actv_df = actv_df.rename(columns={'timestep': 'Timestep', 'mean': 'Mean', 'num_active': 'Number of Active Units', 'mean_not_null': 'Mean of Active Units', 'onset': 'Onset', 'region': 'Region', 'contrast': 'Contrast', 'model': 'Model', 'layer': 'Layer'})\n",
    "actv_df['Timestep'] += 1\n",
    "actv_df['Normalized Activations'] = actv_df.groupby(['Model', 'Layer'], sort=False).apply(\n",
    "    lambda df: df['Mean'] / df.loc[(df.Timestep == 1) & (df.Region=='ul'), 'Mean'].mean()).reset_index(level=['Model', 'Layer'], drop=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "193cff8176a6c2e1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "sns.relplot(data=actv_df[actv_df.Region.isin(['ul', 'lr']) & (actv_df.Layer==1) & (actv_df.Model.isin(['Divisive Norm.', 'Additive']))], \n",
    "            x='Timestep', y='Normalized Activations', hue='Region', palette=['red', 'green'], kind='line', row='Model', style='Contrast', facet_kws={'sharey': False}, height=2.5, aspect=1.5)\n",
    "sns.despine(offset=5)\n",
    "\n",
    "\n",
    "# move legend up a little bit\n",
    "plt.legend(bbox_to_anchor=(0.5, 1.1), loc='upper center', ncol=3, bbox_transform=plt.gcf().transFigure)\n",
    "\n",
    "\n",
    "plt.savefig('figures/attn_013.svg')\n",
    "plt.savefig('figures/attn_013.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "171fdbfa875fe1b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.mkdir('trained_models')\n",
    "actv_df.to_csv('trained_models/video_activation_data.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12675f12d26660e4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Causal Experiments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "898f929610c9faeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# actv_dict = {'contrast': [], 'timestep': [], 'layer': [], 'mean': [], 'num_active': [], 'mean_not_null': [], 'onset': [], 'region': []}\n",
    "res = {'Intervention': [], 'Accuracy': [], 'Image': [], 'Mean': [], 'Model': []}\n",
    "def hook_fn(actv, hook):\n",
    "    # batch map width height\n",
    "    # ratio = actv[..., 26:, 26:].mean() / actv[..., :26, :26].mean()  # big / small\n",
    "    mid = actv.shape[-1] // 2\n",
    "    ratio = actv[..., mid:, mid:].mean(dim=(-1, -2), keepdim=True)/ (actv[..., :mid, :mid].mean(dim=(-1, -2), keepdim=True) + 1e-5)\n",
    "    actv[..., :mid, :mid] *= ratio\n",
    "    actv[..., mid:, mid:] /= ratio + 1e-5\n",
    "    return actv\n",
    "\n",
    "def acc(logits, target, timestep):\n",
    "    logits = logits[:, timestep, :]\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    acc = accuracy(preds, target, task='multiclass', num_classes=10)\n",
    "    return acc\n",
    "        \n",
    "    \n",
    "onset = 2\n",
    "# for contrast in [0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "contrast = 0.6\n",
    "two_image_dataset = TwoImageTemporalDataset('train', onset_img_2=onset, transform=transform,\n",
    "                                img_to_timesteps_transforms=timestep_transforms, contrast_1=0.6, contrast_2=contrast)        \n",
    "loader = DataLoader(two_image_dataset, batch_size=5, shuffle=True, num_workers=0)\n",
    "\n",
    "j = 0\n",
    "for x, y in tqdm(loader):\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    for name, model in models.items():\n",
    "        model.cuda()\n",
    "        logits_clean, cache_clean = model.run_with_cache(x)\n",
    "        res['Intervention'].append(False)\n",
    "        res['Accuracy'].append(float(acc(logits_clean, y[:, onset], onset)))\n",
    "        res['Image'].append('second')\n",
    "        res['Mean'].append(float(cache_clean[f'hks.adapt_0_{onset}'][..., 26:, 26:].mean()))\n",
    "        \n",
    "        res['Intervention'].append(False)\n",
    "        res['Accuracy'].append(float(acc(logits_clean, y[:, 0], onset)))\n",
    "        res['Image'].append('first')\n",
    "        res['Mean'].append(float(cache_clean[f'hks.adapt_0_{onset}'][..., :26, :26].mean()))\n",
    "        \n",
    "        with model.hooks([(f'hks.adapt_0_{onset}', hook_fn)]):\n",
    "            logits_swapped, cache_swapped = model.run_with_cache(x)\n",
    "            res['Intervention'].append(True)\n",
    "            res['Accuracy'].append(float(acc(logits_swapped, y[:, onset], onset)))\n",
    "            res['Image'].append('second')\n",
    "            res['Mean'].append(float(cache_swapped[f'hks.adapt_0_{onset}'][..., 26:, 26:].mean()))\n",
    "            \n",
    "            res['Intervention'].append(True)\n",
    "            res['Accuracy'].append(float(acc(logits_swapped, y[:, 0], onset)))\n",
    "            res['Image'].append('first')          \n",
    "            res['Mean'].append(float(cache_swapped[f'hks.adapt_0_{onset}'][..., :26, :26].mean()))\n",
    "            \n",
    "        res['Model'].append(name)\n",
    "        res['Model'].append(name)\n",
    "        res['Model'].append(name)\n",
    "        res['Model'].append(name)\n",
    "                \n",
    "    j += 1\n",
    "    if j >= 200:\n",
    "        break\n",
    "causal_df = pd.DataFrame(res)\n",
    "causal_df\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1acec8bbc207393e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "onset = 5\n",
    "two_image_dataset = TwoImageTemporalDataset('train', onset_img_2=onset, transform=transform,\n",
    "                                img_to_timesteps_transforms=timestep_transforms, contrast_1=0.6, contrast_2=0.6)        \n",
    "loader = DataLoader(two_image_dataset, batch_size=5, shuffle=True, num_workers=0)\n",
    "\n",
    "j = 0\n",
    "x, y = next(iter(loader))\n",
    "x = x.cuda()\n",
    "y = y.cuda()\n",
    "logits_clean, cache_clean = div_norm_model.run_with_cache(x)\n",
    "        \n",
    "with div_norm_model.hooks([(f'hks.adapt_0_{onset}', hook_fn)]):\n",
    "    logits_swapped, cache_swapped = div_norm_model.run_with_cache(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab8232f216050d07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx = 2\n",
    "map = 14\n",
    "onset = 5\n",
    "clean = cache_clean[f'hks.adapt_0_{onset}'][idx, map].cpu()\n",
    "corr = cache_swapped[f'hks.adapt_0_{onset}'][idx, map].cpu()\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2)\n",
    "\n",
    "ax[0].imshow(clean, vmin=0, vmax=.7)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "\n",
    "ax[1].imshow(corr, vmin=0, vmax=.7)\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "\n",
    "ax[0].annotate('', xy=(1.215, 0.5), xycoords='axes fraction', xytext=(1, 0.5),\n",
    "               arrowprops=dict(arrowstyle=\"->\", lw=3.5))\n",
    "\n",
    "plt.savefig('figures/attn_009.svg')\n",
    "plt.savefig('figures/attn_009.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6214cc6bd0cb5c76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.catplot(data=causal_df, x='Image', y='Accuracy', hue='Intervention', order=['first', 'second'], kind='bar', col='Model', col_order=['Divisive Norm.', 'Additive'], height=3, palette=['grey', 'red'])\n",
    "#sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "#plt.tight_layout()\n",
    "sns.despine(offset=10)\n",
    "\n",
    "plt.savefig('figures/attn_10.svg')\n",
    "plt.savefig('figures/attn_10.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da0cb0cb70fc81e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.barplot(data=causal_df, x='Image', y='Mean', hue='Intervention')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "716d74a234a3a157"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "29faf9492df8cea3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
